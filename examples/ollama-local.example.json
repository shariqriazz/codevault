{
  "comment": "CodeVault configuration for fully local setup with Ollama via OpenAI-compatible API (no API costs)",
  "defaultProvider": "openai",
  "providers": {
    "openai": {
      "baseUrl": "http://localhost:11434/v1",
      "model": "nomic-embed-text",
      "dimensions": 768
    }
  },
  "chatLLM": {
    "openai": {
      "baseUrl": "http://localhost:11434/v1",
      "model": "llama3.1"
    }
  },
  "maxTokens": 8192,
  "rateLimit": {
    "rpm": null,
    "tpm": null
  },
  "encryption": {
    "enabled": false
  }
}
